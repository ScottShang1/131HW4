---
title: "131 Homework3"
author: "Scott Shang (8458655)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

---
title: "131 Homework3"
author: "Scott Shang (8458655)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, echo=FALSE}
library(knitr)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)


## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
```

Question1
```{r}
library("tidyverse")
library("tidymodels")
library(readr)
library(pROC)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
tt=read_csv('titanic.csv')
tt$survived=factor(tt$survived)
tt$survived=relevel(tt$survived,"Yes")
tt$pclass=factor(tt$pclass)
head(tt)
```
```{r}
set.seed(1234)
tt_split=initial_split(tt,prop=0.80,strata=survived)
train=training(tt_split)
test=testing(tt_split)

nrow(tt)
nrow(train)+nrow(test)
colSums(is.na(train))/nrow(train)
```
We showed that the training and testing data sets have the appropriate number of observations. 
We saw that we have missing data on predictors:  age, cabin, embarked. Specific missing proportion is showed above.
We want to use stratified sampling since we are able to stratify our sample by the response variable survive by died or alive and find the difference and relationship between them, and stratified sampling ensures each group of data receive proper representation of each.

Question2
```{r}
tt %>% 
  ggplot(aes(x = survived)) +
  geom_bar()
```
We observe that the outcome variable survived is either Yes or No, with approximately 340 Yes and 570 No. Overall speaking, there are 2/3 more people died comparing to the number of survivor.

Question3
```{r}
cor_tt=train %>%
  dplyr::select("age","sib_sp","parch","fare") %>%
  correlate()

cor_tt %>%
  stretch() %>%
  ggplot(aes(x, y, fill = r)) +
  geom_tile() +
  geom_text(aes(label = as.character(fashion(r))))
```
We want correlation of continuous variables, which are "age","sib_sp","parch",and "fare". Most correlation are pretty weak. "sib_sp" and "parch" have the strongest one with 0.41 positive correlation, which is not that strong. Then, it's the correlation between "sib_sp" and "age", with -0.28 negative correlation.

Question4
```{r}
tt_recipe=recipe(survived~pclass+sex+age+sib_sp+parch+fare,data=train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_impute_linear(age,impute_with=imp_vars(all_predictors())) %>%
  step_interact(terms=~sex_male:fare+age:fare)
```


Question5
```{r}
log_reg=logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
log_wf=workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(tt_recipe)
log_fit=fit(log_wf,train)
```


Question6
```{r}
lda_mod=discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
lda_wf=workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(tt_recipe)
lda_fit=fit(lda_wf,train)
```


Question7
```{r}
qda_mod=discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
qda_wf=workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(tt_recipe)
qda_fit=fit(qda_wf,train)
```


Question8
```{r}
nb_mod=naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel=FALSE) 
nb_wf=workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(tt_recipe)
nb_fit=fit(nb_wf,train)
```

Question 9
```{r}
predictlog=predict(log_fit,new_data=train)
predictlog=bind_cols(predictlog)
predictlda=predict(lda_fit,new_data=train)
predictlda=bind_cols(predictlda)
predictqda=predict(qda_fit,new_data=train)
predictqda=bind_cols(predictqda)
predictnb=predict(nb_fit,new_data=train)
predictnb=bind_cols(predictnb)
```


```{r}
log_acc=augment(log_fit, new_data=train) %>%
  accuracy(truth=survived,estimate=.pred_class)
  
lda_acc=augment(lda_fit, new_data=train) %>%
  accuracy(truth=survived,estimate=.pred_class)
  
qda_acc=augment(qda_fit, new_data=train) %>%
  accuracy(truth=survived,estimate=.pred_class)
  
nb_acc=augment(nb_fit, new_data=train) %>%
  accuracy(truth=survived,estimate=.pred_class)
  
accuracy=c(log_acc$.estimate,lda_acc$.estimate,             nb_acc$.estimate,qda_acc$.estimate)
                
mods=c("Logistic Regression","LDA","Naive Bayes","QDA")
results=tibble(accuracy=accuracy,mods=mods)
results %>% 
  arrange(-accuracy)
```
From the data above, we notice that the logistic regression has the highest accuracy on training data.

Question 10
```{r}
new_log_reg=augment(log_fit,new_data=test) %>%
  accuracy(truth=survived,estimate=.pred_class)
new_log_reg
```


```{r}
augment(log_fit,new_data=test) %>%
  conf_mat(truth=survived,estimate=.pred_class)
```

```{r}
augment(log_fit,new_data=test) %>%
  conf_mat(truth=survived,estimate=.pred_class) %>%
  autoplot(type="heatmap")
```

```{r}
augment(log_fit,new_data=test) %>%
  roc_curve(truth=survived,.pred_Yes) %>%
  autoplot()
```

```{r}
augment(log_fit,new_data=test) %>%
  roc_auc(truth=survived,.pred_Yes)
```
AUC=0.8503

How did the model perform? Compare its training and testing accuracies. If the values differ, why do you think this is so?

I used the logistic regression model. It has 0.8188 accuracy on the training data and 0.7821 accuracy on the testing data, which is slightly worse. We might overfit the model, but overall speaking, it's acceptable.